{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15a743b",
   "metadata": {},
   "source": [
    "# üåº Transfer Learning com VGG16 no Oxford Flowers 102 (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRANSFER LEARNING E FINE-TUNING COM VGG16 - OXFORD FLOWERS 102\n",
    "\n",
    "## üîÅ 1. Instala√ß√£o de depend√™ncias\n",
    "!pip install tensorflow tensorflow_datasets -q\n",
    "\n",
    "## üì• 2. Carregando o Oxford Flowers 102\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True)\n",
    "train_ds, val_ds, test_ds = dataset['train'], dataset['validation'], dataset['test']\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "## üßπ 3. Pr√©-processamento (tamanho e normaliza√ß√£o)\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.one_hot(label, num_classes)\n",
    "\n",
    "train_ds = train_ds.map(format_image).shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n",
    "val_ds = val_ds.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "test_ds = test_ds.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "## üß† 4. Modelo com Transfer Learning (VGG16)\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False  # Congela todas as camadas\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Treinando modelo com Transfer Learning (camadas congeladas)...\")\n",
    "history_tl = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "## üîì 5. Fine-Tuning (descongelando camadas finais)\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Treinando modelo com Fine-Tuning (√∫ltimas camadas descongeladas)...\")\n",
    "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
    "\n",
    "## üìä 6. Avalia√ß√£o no conjunto de teste\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"Acur√°cia final no conjunto de teste: {acc:.4f}\")\n",
    "\n",
    "## üìà 7. Gr√°ficos comparativos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(hist1, hist2, title):\n",
    "    plt.plot(hist1.history['val_accuracy'] + hist2.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.plot(hist1.history['accuracy'] + hist2.history['accuracy'], label='Train Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_tl, history_ft, \"Transfer Learning + Fine-Tuning - VGG16\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
