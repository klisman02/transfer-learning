{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15a743b",
   "metadata": {},
   "source": [
    "# üåº Transfer Learning com VGG16 no Oxford Flowers 102 (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFER LEARNING E FINE-TUNING COM VGG16 - OXFORD FLOWERS 102\n",
    "\n",
    "## üîÅ 1. Instala√ß√£o de depend√™ncias\n",
    "# Instala as bibliotecas TensorFlow e TensorFlow Datasets.\n",
    "# -q: Garante uma instala√ß√£o \"silenciosa\", sem exibir muitas mensagens no terminal.\n",
    "!pip install tensorflow tensorflow_datasets -q\n",
    "\n",
    "## üì• 2. Carregando o Oxford Flowers 102\n",
    "import tensorflow_datasets as tfds # Importa a biblioteca para carregar datasets do TensorFlow\n",
    "\n",
    "# Carrega o dataset 'oxford_flowers102'.\n",
    "# with_info=True: Retorna metadados do dataset (como n√∫mero de classes, etc.).\n",
    "# as_supervised=True: Retorna o dataset no formato (imagem, r√≥tulo).\n",
    "dataset, info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True)\n",
    "\n",
    "# Divide o dataset nas partes de treino, valida√ß√£o e teste.\n",
    "train_ds, val_ds, test_ds = dataset['train'], dataset['validation'], dataset['test']\n",
    "\n",
    "# Obt√©m o n√∫mero total de classes de flores a partir dos metadados do dataset.\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "## üßπ 3. Pr√©-processamento (tamanho e normaliza√ß√£o)\n",
    "import tensorflow as tf # Importa o TensorFlow para opera√ß√µes de manipula√ß√£o de imagem\n",
    "\n",
    "IMG_SIZE = 224 # Define o tamanho que as imagens ter√£o (224x224 pixels), padr√£o para VGG16.\n",
    "BATCH_SIZE = 32 # Define o n√∫mero de imagens processadas por lote (batch) durante o treinamento.\n",
    "\n",
    "# Fun√ß√£o para formatar as imagens e r√≥tulos.\n",
    "def format_image(image, label):\n",
    "    # Redimensiona a imagem para o tamanho desejado (224x224).\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    # Converte os valores dos pixels para float32 e os normaliza para o intervalo [0, 1].\n",
    "    # (Pixels originais s√£o de 0-255, a normaliza√ß√£o ajuda a rede neural a aprender melhor).\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # Converte o r√≥tulo num√©rico para um vetor one-hot encoded (ex: 5 -> [0,0,0,0,1,0,...]).\n",
    "    # Isso √© necess√°rio para a fun√ß√£o de perda 'categorical_crossentropy'.\n",
    "    return image, tf.one_hot(label, num_classes)\n",
    "\n",
    "# Aplica a fun√ß√£o de pr√©-processamento aos datasets e configura o pipeline de dados.\n",
    "# .map(format_image): Aplica a fun√ß√£o format_image a cada elemento do dataset.\n",
    "# .shuffle(1000): Embaralha o dataset de treino para garantir que os lotes sejam diversos. O n√∫mero 1000 √© o tamanho do buffer de embaralhamento.\n",
    "# .batch(BATCH_SIZE): Agrupa os elementos em lotes do tamanho especificado.\n",
    "# .prefetch(1): Permite que os dados sejam pr√©-carregados enquanto o modelo est√° treinando, otimizando a performance.\n",
    "train_ds = train_ds.map(format_image).shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n",
    "val_ds = val_ds.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "test_ds = test_ds.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "## üß† 4. Modelo com Transfer Learning (VGG16)\n",
    "# Importa as classes necess√°rias do Keras.\n",
    "from tensorflow.keras.applications import VGG16 # Para carregar o modelo VGG16 pr√©-treinado.\n",
    "from tensorflow.keras.models import Model # Para criar um modelo funcional a partir de camadas.\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input # Camadas comuns para redes neurais.\n",
    "from tensorflow.keras.optimizers import Adam # Otimizador Adam para ajustar os pesos do modelo.\n",
    "\n",
    "# Carrega o modelo VGG16 pr√©-treinado no dataset ImageNet.\n",
    "# weights='imagenet': Usa os pesos aprendidos no ImageNet.\n",
    "# include_top=False: Exclui as camadas totalmente conectadas finais (cabe√ßalho de classifica√ß√£o) do modelo original.\n",
    "# input_shape: Define a forma de entrada esperada (altura, largura, canais de cor).\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Congela todas as camadas do modelo base VGG16.\n",
    "# Isso significa que os pesos dessas camadas N√ÉO ser√£o atualizados durante o treinamento inicial.\n",
    "# Estamos usando a VGG16 como um extrator de caracter√≠sticas fixo.\n",
    "base_model.trainable = False\n",
    "\n",
    "# Constr√≥i o novo cabe√ßalho de classifica√ß√£o que ser√° adicionado ao modelo base.\n",
    "# Flatten(): Transforma a sa√≠da 3D da VGG16 (mapas de caracter√≠sticas) em um vetor 1D.\n",
    "x = Flatten()(base_model.output)\n",
    "# Dense(256, activation='relu'): Uma camada totalmente conectada com 256 neur√¥nios e fun√ß√£o de ativa√ß√£o ReLU.\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# Dropout(0.5): Desativa aleatoriamente 50% dos neur√¥nios durante o treinamento para evitar overfitting.\n",
    "x = Dropout(0.5)(x)\n",
    "# Dense(num_classes, activation='softmax'): A camada de sa√≠da, com 'num_classes' neur√¥nios.\n",
    "# 'softmax' √© usada para classifica√ß√£o multiclasse, resultando em probabilidades para cada classe.\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Cria o modelo final combinando a VGG16 (congelada) com o novo cabe√ßalho de classifica√ß√£o.\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compila o modelo.\n",
    "# optimizer=Adam(): O otimizador Adam √© usado para ajustar os pesos das camadas trein√°veis.\n",
    "# loss='categorical_crossentropy': Fun√ß√£o de perda para problemas de classifica√ß√£o multiclasse com r√≥tulos one-hot.\n",
    "# metrics=['accuracy']: Monitora a acur√°cia (precis√£o) durante o treinamento.\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Treinando modelo com Transfer Learning (camadas congeladas)...\")\n",
    "# Treina o modelo.\n",
    "# train_ds: Dataset de treino.\n",
    "# validation_data=val_ds: Dataset de valida√ß√£o para monitorar o desempenho em dados n√£o vistos.\n",
    "# epochs=10: N√∫mero de vezes que o modelo ir√° percorrer todo o dataset de treino.\n",
    "history_tl = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "## üîì 5. Fine-Tuning (descongelando camadas finais)\n",
    "# Descongela todas as camadas do modelo base para prepar√°-las para o fine-tuning.\n",
    "base_model.trainable = True\n",
    "\n",
    "# Congela novamente todas as camadas do modelo base, EXCETO as √∫ltimas 4.\n",
    "# A ideia √© que as camadas mais profundas (finais) do VGG16 possam ser ajustadas aos dados espec√≠ficos das flores,\n",
    "# enquanto as camadas iniciais (que capturam caracter√≠sticas gen√©ricas) permanecem fixas.\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompila o modelo com uma taxa de aprendizado muito menor para o fine-tuning.\n",
    "# Uma taxa de aprendizado menor evita \"desaprender\" o conhecimento pr√©-existente e permite ajustes finos.\n",
    "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Treinando modelo com Fine-Tuning (√∫ltimas camadas descongeladas)...\")\n",
    "# Continua o treinamento do modelo por mais 5 √©pocas.\n",
    "# Agora, tanto o cabe√ßalho de classifica√ß√£o quanto as √∫ltimas 4 camadas da VGG16 ser√£o ajustados.\n",
    "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
    "\n",
    "## üìä 6. Avalia√ß√£o no conjunto de teste\n",
    "# Avalia o desempenho final do modelo no conjunto de teste (dados totalmente novos).\n",
    "# Isso d√° uma estimativa realista de como o modelo se comportar√° em dados do mundo real.\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"Acur√°cia final no conjunto de teste: {acc:.4f}\")\n",
    "\n",
    "## üìà 7. Gr√°ficos comparativos\n",
    "import matplotlib.pyplot as plt # Importa a biblioteca para criar gr√°ficos\n",
    "\n",
    "# Fun√ß√£o para plotar os hist√≥ricos de acur√°cia de treino e valida√ß√£o.\n",
    "def plot_history(hist1, hist2, title):\n",
    "    # Combina os hist√≥ricos de acur√°cia de valida√ß√£o das duas fases (transfer learning e fine-tuning).\n",
    "    plt.plot(hist1.history['val_accuracy'] + hist2.history['val_accuracy'], label='Val Accuracy')\n",
    "    # Combina os hist√≥ricos de acur√°cia de treino das duas fases.\n",
    "    plt.plot(hist1.history['accuracy'] + hist2.history['accuracy'], label='Train Accuracy')\n",
    "    plt.title(title) # Define o t√≠tulo do gr√°fico.\n",
    "    plt.xlabel('Epochs') # R√≥tulo do eixo X.\n",
    "    plt.ylabel('Accuracy') # R√≥tulo do eixo Y.\n",
    "    plt.legend() # Mostra a legenda das linhas.\n",
    "    plt.grid() # Adiciona uma grade ao gr√°fico.\n",
    "    plt.show() # Exibe o gr√°fico.\n",
    "\n",
    "# Chama a fun√ß√£o para plotar os resultados combinados.\n",
    "plot_history(history_tl, history_ft, \"Transfer Learning + Fine-Tuning - VGG16\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
